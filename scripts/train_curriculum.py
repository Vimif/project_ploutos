#!/usr/bin/env python3
"""
üéì CURRICULUM LEARNING POUR PLOUTOS
Entra√Ænement progressif : Simple ‚Üí Complexe

Avec auto-optimisation rapide optionnelle (15 trials)

Usage:
    python3 scripts/train_curriculum.py --stage 1                     # Sans optimisation
    python3 scripts/train_curriculum.py --stage 1 --auto-optimize     # Avec optimisation rapide
    python3 scripts/train_curriculum.py --stage 2 --load-model models/stage1_spy_final
"""

import sys
sys.path.insert(0, '.')

import os
import json
import wandb
import numpy as np
import pandas as pd
from datetime import datetime
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback
from core.data_fetcher import UniversalDataFetcher
from core.universal_environment import UniversalTradingEnv

# ============================================================================
# PARAMS PR√â-CALIBR√âS (issus d'exp√©riences pr√©c√©dentes)
# ============================================================================

CALIBRATED_PARAMS = {
    'stage1': {
        'name': 'Mono-Asset (SPY)',
        'tickers': ['SPY'],
        'timesteps': 3_000_000,
        'n_envs': 4,
        'learning_rate': 1e-4,
        'n_steps': 2048,
        'batch_size': 256,
        'n_epochs': 5,              # ‚úÖ 10 ‚Üí 5 (anti-overfitting)
        'gamma': 0.99,
        'gae_lambda': 0.95,
        'clip_range': 0.2,
        'ent_coef': 0.05,           # ‚úÖ 0.01 ‚Üí 0.05 (plus d'exploration)
        'vf_coef': 0.5,
        'max_grad_norm': 0.3,       # ‚úÖ 0.5 ‚Üí 0.3 (clipping strict)
        'policy_kwargs': {'net_arch': [512, 512, 512]},
        'target_sharpe': 1.0
    },
    'stage2': {
        'name': 'Multi-Asset ETFs',
        'tickers': ['SPY', 'QQQ', 'IWM'],
        'timesteps': 5_000_000,
        'n_envs': 8,
        'learning_rate': 5e-5,
        'n_steps': 2048,
        'batch_size': 512,
        'n_epochs': 5,              # ‚úÖ 10 ‚Üí 5
        'gamma': 0.99,
        'gae_lambda': 0.95,
        'clip_range': 0.2,
        'ent_coef': 0.02,           # ‚úÖ 0.005 ‚Üí 0.02
        'vf_coef': 0.5,
        'max_grad_norm': 0.3,       # ‚úÖ 0.5 ‚Üí 0.3
        'policy_kwargs': {'net_arch': [512, 512, 512]},
        'target_sharpe': 1.3
    },
    'stage3': {
        'name': 'Actions Complexes',
        'tickers': ['NVDA', 'MSFT', 'AAPL', 'GOOGL', 'AMZN'],
        'timesteps': 10_000_000,
        'n_envs': 16,
        'learning_rate': 3e-5,
        'n_steps': 4096,
        'batch_size': 1024,
        'n_epochs': 5,              # ‚úÖ 10 ‚Üí 5
        'gamma': 0.99,
        'gae_lambda': 0.95,
        'clip_range': 0.2,
        'ent_coef': 0.01,           # ‚úÖ 0.001 ‚Üí 0.01
        'vf_coef': 0.5,
        'max_grad_norm': 0.3,       # ‚úÖ 0.5 ‚Üí 0.3
        'policy_kwargs': {'net_arch': [512, 512, 512]},
        'target_sharpe': 1.5
    }
}

# ============================================================================
# AUTO-OPTIMISATION RAPIDE (15 trials)
# ============================================================================

def quick_optimize(tickers, base_params, stage_name, n_trials=15):
    """
    Auto-optimisation RAPIDE (15 trials au lieu de 50)
    Optimise seulement les 3 params les plus critiques
    
    Args:
        tickers: Liste de tickers pour optimisation
        base_params: Params de d√©part
        stage_name: Nom du stage (pour logs)
        n_trials: Nombre d'essais (15 par d√©faut)
        
    Returns:
        dict: Params optimis√©s
    """
    
    print("\n" + "="*80)
    print(f"‚ö° AUTO-OPTIMISATION RAPIDE - {stage_name}")
    print("="*80)
    print(f"  Trials       : {n_trials}")
    print(f"  Dur√©e estim√©e : ~30-40 minutes")
    print(f"  Assets test  : {', '.join(tickers[:2])}")
    print(f"  Params test√©s : learning_rate, n_steps, ent_coef\n")
    
    import optuna
    from optuna.pruners import MedianPruner
    
    # Charger donn√©es
    data = {}
    for ticker in tickers[:2]:  # Max 2 tickers pour rapidit√©
        cache_file = f'data_cache/{ticker}.csv'
        if os.path.exists(cache_file):
            data[ticker] = pd.read_csv(cache_file, index_col=0, parse_dates=True)
    
    if len(data) == 0:
        print("‚ö†Ô∏è  Aucune donn√©e disponible, skip optimisation")
        return base_params
    
    def objective(trial):
        """Objective Optuna - teste seulement 3 params critiques"""
        
        # Partir des params de base
        params = base_params.copy()
        
        # Optimiser SEULEMENT les 3 plus importants
        params['learning_rate'] = trial.suggest_float(
            'learning_rate',
            base_params['learning_rate'] * 0.5,
            base_params['learning_rate'] * 2.0,
            log=True
        )
        
        params['n_steps'] = trial.suggest_categorical(
            'n_steps',
            [base_params['n_steps'] // 2, base_params['n_steps'], base_params['n_steps'] * 2]
        )
        
        params['ent_coef'] = trial.suggest_float(
            'ent_coef',
            base_params['ent_coef'] * 0.1,
            base_params['ent_coef'] * 10.0,
            log=True
        )
        
        # √âvaluer sur 2 tickers
        sharpes = []
        
        for ticker, df in data.items():
            try:
                # Cr√©er env simple
                env = UniversalTradingEnv(
                    data={ticker: df},
                    initial_balance=10000,
                    commission=0.001,
                    max_steps=500
                )
                
                # Entra√Ænement court (100k timesteps)
                policy_kwargs = params.pop('policy_kwargs', {'net_arch': [256, 256]})
                
                model = PPO(
                    'MlpPolicy',
                    env,
                    verbose=0,
                    device='cuda',
                    policy_kwargs=policy_kwargs,
                    **{k: v for k, v in params.items() if k != 'target_sharpe'}
                )
                
                model.learn(total_timesteps=100_000)
                
                # Backtest rapide
                obs, _ = env.reset()
                values = []
                done = False
                
                for _ in range(300):
                    action, _ = model.predict(obs, deterministic=True)
                    obs, reward, terminated, truncated, info = env.step(action)
                    values.append(info['portfolio_value'])
                    done = terminated or truncated
                    if done:
                        break
                
                # Calculer Sharpe
                if len(values) > 10:
                    df_val = pd.DataFrame({'value': values})
                    df_val['ret'] = df_val['value'].pct_change().fillna(0)
                    
                    mean_ret = df_val['ret'].mean()
                    std_ret = df_val['ret'].std()
                    
                    if std_ret > 0:
                        sharpe = (mean_ret / std_ret) * np.sqrt(252)
                        sharpes.append(sharpe)
                
                # Restaurer policy_kwargs
                params['policy_kwargs'] = policy_kwargs
                
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Trial {trial.number} √©chec sur {ticker}: {str(e)[:50]}")
                continue
        
        if len(sharpes) == 0:
            return float('-inf')
        
        mean_sharpe = float(np.mean(sharpes))
        
        # Pruning
        trial.report(mean_sharpe, step=0)
        if trial.should_prune():
            raise optuna.TrialPruned()
        
        return mean_sharpe
    
    # Cr√©er √©tude
    study = optuna.create_study(
        direction='maximize',
        sampler=optuna.samplers.TPESampler(seed=42),
        pruner=MedianPruner()
    )
    
    # Optimiser
    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
    
    # Merger meilleurs params avec base
    optimized_params = base_params.copy()
    optimized_params.update(study.best_params)
    
    print(f"\n‚úÖ OPTIMISATION TERMIN√âE")
    print(f"  Sharpe am√©lior√©      : {study.best_value:.3f}")
    print(f"  Learning rate     : {base_params['learning_rate']:.6f} ‚Üí {optimized_params['learning_rate']:.6f}")
    print(f"  N steps           : {base_params['n_steps']} ‚Üí {optimized_params['n_steps']}")
    print(f"  Ent coef          : {base_params['ent_coef']:.6f} ‚Üí {optimized_params['ent_coef']:.6f}\n")
    
    # Sauvegarder
    os.makedirs(f'models/stage{stage_name[-1]}', exist_ok=True)
    with open(f'models/stage{stage_name[-1]}/optimized_params.json', 'w') as f:
        json.dump(optimized_params, f, indent=2)
    
    return optimized_params

# ============================================================================
# FONCTIONS UTILITAIRES
# ============================================================================

def print_banner(text):
    """Affiche une banni√®re"""
    print("\n" + "="*80)
    print(f"  {text}")
    print("="*80 + "\n")

def make_env(data_dict):
    """Cr√©e un environnement de trading"""
    def _init():
        return UniversalTradingEnv(
            data=data_dict,
            initial_balance=10000,
            commission=0.001,
            max_steps=1000
        )
    return _init

def calculate_sharpe(model, data_dict, episodes=10):
    """
    Calcule le Sharpe Ratio du mod√®le
    
    ‚úÖ FIX : Ajuste max_steps dynamiquement pour √©viter ValueError
    """
    returns = []
    
    # ‚úÖ Calculer la taille minimale des donn√©es
    data_length = min(len(df) for df in data_dict.values())
    
    # ‚úÖ Si donn√©es trop courtes, skip √©valuation
    if data_length < 150:
        print(f"\n‚ö†Ô∏è  Donn√©es de test trop courtes ({data_length} lignes), skip √©valuation Sharpe")
        return 0.0
    
    # ‚úÖ Ajuster max_steps : min(500, data_length - 110)
    # Laisser marge de 110 (100 pour features + 10 buffer)
    adjusted_max_steps = min(500, data_length - 110)
    
    for _ in range(episodes):
        env = UniversalTradingEnv(
            data=data_dict,
            initial_balance=10000,
            commission=0.001,
            max_steps=adjusted_max_steps  # ‚úÖ Dynamique
        )
        
        obs, _ = env.reset()
        done = False
        episode_return = 0
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            episode_return += reward
        
        returns.append(episode_return)
    
    returns = np.array(returns)
    
    if len(returns) == 0 or returns.std() == 0:
        return 0
    
    sharpe = (returns.mean() / returns.std()) * np.sqrt(252)
    return sharpe

# ============================================================================
# TRAIN STAGE G√âN√âRIQUE
# ============================================================================

def train_stage(stage_num, prev_model_path=None, auto_optimize=False):
    """
    Entra√Æne un stage du curriculum
    
    Args:
        stage_num: Num√©ro du stage (1, 2, ou 3)
        prev_model_path: Chemin vers mod√®le pr√©c√©dent (transfer learning)
        auto_optimize: Si True, lance optimisation rapide avant entra√Ænement
    """
    
    stage_key = f'stage{stage_num}'
    config = CALIBRATED_PARAMS[stage_key].copy()
    
    print_banner(f"üéì STAGE {stage_num} : {config['name']}")
    
    # R√©cup√©rer donn√©es
    print("üì• T√©l√©chargement des donn√©es...")
    fetcher = UniversalDataFetcher()
    data = fetcher.bulk_fetch(config['tickers'], interval='1h')
    
    print(f"‚úÖ {len(data)}/{len(config['tickers'])} tickers r√©cup√©r√©s")
    
    # Auto-optimisation optionnelle
    if auto_optimize:
        config = quick_optimize(
            tickers=config['tickers'],
            base_params=config,
            stage_name=f"Stage {stage_num}",
            n_trials=15
        )
    else:
        print("\nüìã Utilisation params pr√©-calibr√©s (pas d'optimisation)\n")
    
    # Initialiser W&B
    wandb.init(
        project="Ploutos_Curriculum",
        name=f"Stage{stage_num}_{config['name'].replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M')}",
        config=config
    )
    
    # Cr√©er environnements
    print("üè≠ Cr√©ation des environnements...")
    env = SubprocVecEnv([make_env(data) for _ in range(config['n_envs'])])
    
    # Charger mod√®le pr√©c√©dent ou cr√©er nouveau
    if prev_model_path and os.path.exists(f"{prev_model_path}.zip"):
        print(f"\nüîÑ Transfer Learning depuis : {prev_model_path}")
        model = PPO.load(prev_model_path, env=env, device='cuda')
        model.learning_rate = config['learning_rate']
    else:
        print("\nüß† Cr√©ation d'un nouveau mod√®le...")
        
        policy_kwargs = config.pop('policy_kwargs')
        target_sharpe = config.pop('target_sharpe')
        timesteps = config.pop('timesteps')
        n_envs = config.pop('n_envs')
        name = config.pop('name')
        tickers = config.pop('tickers')
        
        model = PPO(
            'MlpPolicy',
            env,
            verbose=1,
            tensorboard_log=f'./logs/{stage_key}',
            device='cuda',
            policy_kwargs=policy_kwargs,
            **config
        )
        
        # Restaurer pour la suite
        config['policy_kwargs'] = policy_kwargs
        config['target_sharpe'] = target_sharpe
        config['timesteps'] = timesteps
        config['n_envs'] = n_envs
        config['name'] = name
        config['tickers'] = tickers
    
    # Callbacks
    os.makedirs(f'models/{stage_key}', exist_ok=True)
    checkpoint_callback = CheckpointCallback(
        save_freq=50000 * stage_num,
        save_path=f'./models/{stage_key}',
        name_prefix=f'ploutos_{stage_key}'
    )
    
    # Entra√Ænement
    print(f"\nüöÄ Entra√Ænement : {config['timesteps']:,} timesteps...")
    print(f"‚è±Ô∏è  Dur√©e estim√©e : ~{config['timesteps'] // 500_000} heures sur RTX 3080\n")
    
    model.learn(
        total_timesteps=config['timesteps'],
        callback=checkpoint_callback,
        progress_bar=True
    )
    
    # Sauvegarder
    model_path = f'models/{stage_key}_final'
    model.save(model_path)
    print(f"\n‚úÖ Mod√®le sauvegard√© : {model_path}.zip")
    
    # √âvaluation
    print("\nüìä √âvaluation du mod√®le...")
    
    # ‚úÖ FIX : Utiliser 20% des donn√©es pour test (au lieu de 1000 lignes fixes)
    data_length = min(len(df) for df in data.values())
    test_size = max(200, int(data_length * 0.2))  # Min 200, max 20%
    
    print(f"  Taille donn√©es test : {test_size} lignes")
    
    test_data = {ticker: df.iloc[-test_size:] for ticker, df in data.items()}
    
    sharpe = calculate_sharpe(model, test_data, episodes=10)
    print(f"\nüìà Sharpe Ratio : {sharpe:.2f}")
    print(f"üéØ Objectif      : {config['target_sharpe']:.2f}")
    
    success = sharpe >= config['target_sharpe']
    
    if success:
        print(f"\n‚úÖ STAGE {stage_num} R√âUSSI !")
    else:
        print(f"\n‚ö†Ô∏è  Sharpe insuffisant, mais on continue...")
    
    wandb.log({
        'stage': stage_num,
        'sharpe_ratio': sharpe,
        'target_sharpe': config['target_sharpe'],
        'success': success
    })
    
    wandb.finish()
    env.close()
    
    return model_path, sharpe

# ============================================================================
# MAIN
# ============================================================================

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Curriculum Learning pour Ploutos',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  python3 scripts/train_curriculum.py --stage 1
  python3 scripts/train_curriculum.py --stage 1 --auto-optimize
  python3 scripts/train_curriculum.py --stage 2 --load-model models/stage1_final
  python3 scripts/train_curriculum.py --stage 3 --load-model models/stage2_final --auto-optimize
        """
    )
    
    parser.add_argument('--stage', type=int, required=True, choices=[1, 2, 3],
                        help='Stage √† ex√©cuter (1, 2, ou 3)')
    parser.add_argument('--load-model', type=str, default=None,
                        help='Chemin vers mod√®le pr√©c√©dent pour transfer learning')
    parser.add_argument('--auto-optimize', action='store_true',
                        help='Active auto-optimisation rapide (15 trials, +30min)')
    
    args = parser.parse_args()
    
    print("\n" + "="*80)
    print("üéì PLOUTOS CURRICULUM LEARNING")
    print("="*80)
    print(f"\n‚è∞ D√©but : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üìä Stage : {args.stage}")
    print(f"‚ö° Auto-optimize : {'OUI (+30min)' if args.auto_optimize else 'NON'}\n")
    
    # D√©terminer mod√®le pr√©c√©dent par d√©faut
    if args.load_model is None and args.stage > 1:
        args.load_model = f'models/stage{args.stage - 1}_final'
        print(f"üîó Transfer learning automatique depuis : {args.load_model}\n")
    
    # Ex√©cution
    model_path, sharpe = train_stage(
        stage_num=args.stage,
        prev_model_path=args.load_model,
        auto_optimize=args.auto_optimize
    )
    
    print(f"\n‚è∞ Fin : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"‚úÖ Mod√®le final : {model_path}.zip")
    print(f"üìä Sharpe Ratio : {sharpe:.2f}")
    
    # Sugg√©rer prochaine √©tape
    if args.stage < 3 and sharpe > 0:
        print(f"\nüí° PROCHAINE √âTAPE : python3 scripts/train_curriculum.py --stage {args.stage + 1}")
        if args.auto_optimize:
            print("               Ou avec optimisation : --auto-optimize")
