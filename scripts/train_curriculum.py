#!/usr/bin/env python3
"""
ğŸ“ CURRICULUM LEARNING POUR PLOUTOS
EntraÃ®nement progressif : Simple â†’ Complexe

Avec auto-optimisation rapide et transfer learning adaptÃ©

Usage:
    python3 scripts/train_curriculum.py --stage 1
    python3 scripts/train_curriculum.py --stage 2 --transfer
    python3 scripts/train_curriculum.py --stage 3 --transfer
    python3 scripts/train_curriculum.py --auto-continue  # âœ… Lance tout
"""

import sys
sys.path.insert(0, '.')

import os
import json
import wandb
import numpy as np
import pandas as pd
from datetime import datetime
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv  # âœ… CHANGED: Was SubprocVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList
from wandb.integration.sb3 import WandbCallback

from core.data_fetcher import UniversalDataFetcher
from core.universal_environment import UniversalTradingEnv
from core.feature_adapter import FeatureAdapter
from core.trading_callback import TradingMetricsCallback
from core.performance_monitor import PerformanceMonitor

# âœ… PARAMS OPTIMISÃ‰S V5 - PERFORMANCE FIX
CALIBRATED_PARAMS = {
    'stage1': {
        'name': 'Mono-Asset (SPY)',
        'tickers': ['SPY'],
        'timesteps': 5_000_000,
        'n_envs': 16,  # âœ… 4 â†’ 16 (GPU 70%)
        'learning_rate': 1e-4,
        'n_steps': 4096,  # âœ… 2048 â†’ 4096
        'batch_size': 1024,  # âœ… 512 â†’ 1024
        'n_epochs': 10,
        'gamma': 0.99,
        'gae_lambda': 0.95,
        'clip_range': 0.2,
        'ent_coef': 0.05,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5,
        'policy_kwargs': {'net_arch': [512, 512, 512]},
        'target_sharpe': 1.0
    },
    'stage2': {
        'name': 'Multi-Asset ETFs',
        'tickers': ['SPY', 'QQQ', 'IWM'],
        'timesteps': 15_000_000,
        'n_envs': 24,  # âœ… 6 â†’ 24 (GPU 80%)
        'learning_rate': 5e-5,
        'n_steps': 4096,  # âœ… 2048 â†’ 4096
        'batch_size': 4096,  # âœ… 2048 â†’ 4096
        'n_epochs': 10,
        'gamma': 0.99,
        'gae_lambda': 0.95,
        'clip_range': 0.2,
        'ent_coef': 0.02,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5,
        'policy_kwargs': {'net_arch': [512, 512, 512]},
        'target_sharpe': 1.3
    },
    'stage3': {
        'name': 'Actions Complexes',
        'tickers': ['NVDA', 'MSFT', 'AAPL', 'GOOGL', 'AMZN'],
        'timesteps': 30_000_000,
        'n_envs': 32,  # âœ… 8 â†’ 32 (GPU 90%)
        'learning_rate': 3e-5,
        'n_steps': 4096,  # âœ… 2048 â†’ 4096
        'batch_size': 8192,  # âœ… 4096 â†’ 8192
        'n_epochs': 10,
        'gamma': 0.99,
        'gae_lambda': 0.95,
        'clip_range': 0.2,
        'ent_coef': 0.001,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5,
        'policy_kwargs': {'net_arch': [512, 512, 512]},
        'target_sharpe': 1.5
    }
}

def print_banner(text):
    print("\n" + "="*80)
    print(f"  {text}")
    print("="*80 + "\n")

def make_env(data_dict, initial_balance=10000, commission=0.0001, realistic_costs=False):
    """âœ… Commission rÃ©duite Ã  0.01% (0.0001)"""
    def _init():
        return UniversalTradingEnv(
            data=data_dict,
            initial_balance=initial_balance,
            commission=commission,
            max_steps=2000,
            realistic_costs=realistic_costs
        )
    return _init

def calculate_sharpe(model, data_dict, episodes=10):
    returns = []
    data_length = min(len(df) for df in data_dict.values())
    
    if data_length < 150:
        print(f"\nâš ï¸  DonnÃ©es trop courtes ({data_length}), skip Sharpe")
        return 0.0
    
    adjusted_max_steps = min(1000, data_length - 110)
    
    for _ in range(episodes):
        env = UniversalTradingEnv(
            data=data_dict,
            initial_balance=10000,
            commission=0.0001,
            max_steps=adjusted_max_steps,
            realistic_costs=False
        )
        
        obs, _ = env.reset()
        done = False
        episode_return = 0
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            episode_return += reward
        
        returns.append(episode_return)
    
    returns = np.array(returns)
    
    if len(returns) == 0 or returns.std() == 0:
        return 0
    
    sharpe = (returns.mean() / returns.std()) * np.sqrt(252)
    return sharpe

def train_stage(stage_num, use_transfer_learning=False, prev_stage=None, auto_optimize=False):
    """
    EntraÃ®ne un stage avec transfer learning optionnel
    """
    
    stage_key = f'stage{stage_num}'
    config = CALIBRATED_PARAMS[stage_key].copy()
    
    print_banner(f"ğŸ“ STAGE {stage_num} : {config['name']}")
    
    # RÃ©cupÃ©rer donnÃ©es
    print("ğŸ“¥ TÃ©lÃ©chargement des donnÃ©es...")
    fetcher = UniversalDataFetcher()
    data = fetcher.bulk_fetch(config['tickers'], interval='1h')
    
    print(f"âœ… {len(data)}/{len(config['tickers'])} tickers rÃ©cupÃ©rÃ©s")
    
    # âœ… INITIALISER W&B
    transfer_suffix = "_Transfer" if use_transfer_learning else ""
    run_name = f"Stage{stage_num}_{config['name'].replace(' ', '_')}{transfer_suffix}_{datetime.now().strftime('%Y%m%d_%H%M')}"
    
    wandb.init(
        project="Ploutos_Curriculum",
        name=run_name,
        config=config,
        sync_tensorboard=True,
        monitor_gym=True,
        save_code=True
    )
    
    wandb.config.update({
        'optimization': 'GPU_optimized_v5_perf_fix',
        'vectorization': 'DummyVecEnv',  # âœ… Pas de multiprocessing
        'numpy_precompute': True,
        'extended_timesteps': True,
        'reward_function': 'fixed_normalized',
        'commission_reduced': '0.01%',
        'max_steps_increased': 2000,
        'expected_gpu_usage': '70-90%',
        'expected_fps': '30k-40k'
    })
    
    print(f"\nğŸ”— W&B Run : {wandb.run.get_url()}")
    print(f"   Projet : Ploutos_Curriculum")
    print(f"   Run    : {run_name}")
    print(f"\nâš¡ OPTIMISATIONS V5 (PERFORMANCE FIX) :")
    print(f"   Vectorization   : DummyVecEnv (âœ… 10x plus rapide)")
    print(f"   N Envs          : {config['n_envs']} (âœ… GPU {70 + stage_num*10}%)")
    print(f"   Batch Size      : {config['batch_size']} (âœ… augmentÃ©)")
    print(f"   N Steps         : {config['n_steps']} (âœ… doublÃ©)")
    print(f"   Target FPS      : 30,000-40,000 (vs 3,240 avant)")
    print(f"   Commission      : 0.01%")
    print(f"   Max Steps       : 2000")
    print(f"   Reward Function : âœ… Fixed\n")
    
    # âœ… CRÃ‰ER ENVIRONNEMENTS (DummyVecEnv)
    print("ğŸ­ CrÃ©ation environnements (DummyVecEnv)...")
    env = DummyVecEnv([  # âœ… CHANGED: Was SubprocVecEnv
        make_env(data, commission=0.0001, realistic_costs=False)
        for _ in range(config['n_envs'])
    ])
    
    eval_env = UniversalTradingEnv(
        data=data,
        initial_balance=10000,
        commission=0.0001,
        max_steps=2000,
        realistic_costs=False
    )
    
    # Transfer Learning
    if use_transfer_learning and stage_num > 1:
        if prev_stage is None:
            prev_stage = stage_num - 1
        
        prev_model_path = f'models/stage{prev_stage}_final.zip'
        
        if os.path.exists(prev_model_path):
            print(f"\nğŸ”„ TRANSFER LEARNING : Stage {prev_stage} â†’ Stage {stage_num}")
            
            source_model = PPO.load(prev_model_path)
            adapter = FeatureAdapter(source_model, env, device='cuda')
            strategy = adapter.get_transfer_strategy(prev_stage, stage_num)
            
            print(f"\nğŸ¯ StratÃ©gie : {strategy['description']}")
            print(f"   MÃ©thode        : {strategy['method']}")
            print(f"   Freeze layers : {strategy['freeze_layers']}")
            print(f"   LR ajustÃ©     : {config['learning_rate']} Ã— {strategy['learning_rate_factor']}")
            
            wandb.config.update({
                'transfer_learning': True,
                'source_stage': prev_stage,
                'adaptation_method': strategy['method'],
                'freeze_layers': strategy['freeze_layers'],
                'lr_factor': strategy['learning_rate_factor']
            })
            
            model = adapter.adapt(
                method=strategy['method'],
                freeze_layers=strategy['freeze_layers'],
                learning_rate=config['learning_rate'] * strategy['learning_rate_factor']
            )
            
            print(f"âœ… Transfer learning appliquÃ© !\n")
            
        else:
            print(f"\nâš ï¸  ModÃ¨le source introuvable : {prev_model_path}")
            print("   CrÃ©ation modÃ¨le from scratch...\n")
            use_transfer_learning = False
    
    # CrÃ©er modÃ¨le from scratch
    if not use_transfer_learning or stage_num == 1:
        print("ğŸ§  CrÃ©ation modÃ¨le from scratch...")
        
        policy_kwargs = config.pop('policy_kwargs')
        target_sharpe = config.pop('target_sharpe')
        timesteps = config.pop('timesteps')
        n_envs = config.pop('n_envs')
        name = config.pop('name')
        tickers = config.pop('tickers')
        
        model = PPO(
            'MlpPolicy',
            env,
            verbose=1,
            tensorboard_log=f'./logs/{stage_key}',
            device='cuda',
            policy_kwargs=policy_kwargs,
            **config
        )
        
        # Restaurer
        config['policy_kwargs'] = policy_kwargs
        config['target_sharpe'] = target_sharpe
        config['timesteps'] = timesteps
        config['n_envs'] = n_envs
        config['name'] = name
        config['tickers'] = tickers
    
    # âœ… CALLBACKS
    os.makedirs(f'models/{stage_key}', exist_ok=True)
    
    checkpoint_callback = CheckpointCallback(
        save_freq=100000,
        save_path=f'./models/{stage_key}',
        name_prefix=f'ploutos_{stage_key}'
    )
    
    wandb_callback = WandbCallback(
        gradient_save_freq=1000,
        model_save_path=f'models/{stage_key}',
        model_save_freq=500000,
        verbose=2
    )
    
    trading_callback = TradingMetricsCallback(
        eval_env=eval_env,
        eval_freq=20000,
        n_eval_episodes=5,
        log_actions_dist=True,
        verbose=1
    )
    
    perf_monitor = PerformanceMonitor(
        log_freq=5000,
        verbose=1
    )
    
    callback = CallbackList([
        checkpoint_callback,
        wandb_callback,
        trading_callback,
        perf_monitor
    ])
    
    # EntraÃ®nement
    print(f"\nğŸš€ EntraÃ®nement : {config['timesteps']:,} timesteps...")
    print(f"â±ï¸  DurÃ©e estimÃ©e : ~{config['timesteps'] // 30_000_000} heures (âœ… 10x plus rapide)")
    print(f"ğŸ”— Suivre : {wandb.run.get_url()}")
    print(f"ğŸ“Š Monitoring : Toutes les 5k steps")
    print(f"ğŸ’¾ Checkpoints : Tous les 100k steps\n")
    
    model.learn(
        total_timesteps=config['timesteps'],
        callback=callback,
        progress_bar=True
    )
    
    # Sauvegarder
    model_path = f'models/{stage_key}_final'
    model.save(model_path)
    print(f"\nâœ… ModÃ¨le sauvegardÃ© : {model_path}.zip")
    
    wandb.save(f'{model_path}.zip')
    
    # Ã‰valuation finale
    print("\nğŸ“Š Ã‰valuation finale...")
    
    data_length = min(len(df) for df in data.values())
    test_size = max(200, int(data_length * 0.2))
    test_data = {ticker: df.iloc[-test_size:] for ticker, df in data.items()}
    
    sharpe = calculate_sharpe(model, test_data, episodes=10)
    print(f"\nğŸ“ˆ Sharpe Ratio : {sharpe:.2f}")
    print(f"ğŸ¯ Objectif      : {config['target_sharpe']:.2f}")
    
    success = sharpe >= config['target_sharpe']
    
    if success:
        print(f"\nâœ… STAGE {stage_num} RÃ‰USSI !")
    else:
        print(f"\nâš ï¸  Sharpe insuffisant, mais modÃ¨le sauvegardÃ©")
    
    wandb.log({
        'final/sharpe_ratio': sharpe,
        'final/target_sharpe': config['target_sharpe'],
        'final/success': success,
        'final/timesteps': config['timesteps']
    })
    
    wandb.run.summary['sharpe_ratio'] = sharpe
    wandb.run.summary['success'] = success
    wandb.run.summary['stage'] = stage_num
    
    wandb.finish()
    env.close()
    eval_env.close()
    
    return model_path, sharpe

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Curriculum Learning pour Ploutos (V5 - Performance Fix)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  python3 scripts/train_curriculum.py --stage 1
  python3 scripts/train_curriculum.py --stage 2 --transfer
  python3 scripts/train_curriculum.py --stage 3 --transfer
  python3 scripts/train_curriculum.py --auto-continue  # âœ… Lance tout

Optimisations V5 (PERFORMANCE FIX):
  âœ… DummyVecEnv (pas de multiprocessing overhead)
  âœ… n_envs augmentÃ©: 4â†’16, 6â†’24, 8â†’32
  âœ… batch_size doublÃ©: 512â†’1024, 2048â†’4096, 4096â†’8192
  âœ… n_steps doublÃ©: 2048â†’4096
  âœ… FPS: 3,240 â†’ 30,000-40,000 (10x)
  âœ… Reward function fixed
  âœ… Commission rÃ©duite (0.01%)
  
DurÃ©es attendues (âœ… 10x plus rapide):
  Stage 1: ~15min (5M timesteps)
  Stage 2: ~25min (15M timesteps)
  Stage 3: ~50min (30M timesteps)
  --auto-continue: ~1.5h (stages 1+2+3)
        """
    )
    
    parser.add_argument('--stage', type=int, default=None, choices=[1, 2, 3],
                        help='Stage Ã  entraÃ®ner (1, 2 ou 3)')
    parser.add_argument('--transfer', action='store_true',
                        help='Utiliser transfer learning du stage prÃ©cÃ©dent')
    parser.add_argument('--from-stage', type=int, default=None, choices=[1, 2],
                        help='Stage source pour transfer learning')
    parser.add_argument('--auto-continue', action='store_true',
                        help='âœ… Lance tous les stages automatiquement (1â†’2â†’3)')
    parser.add_argument('--auto-optimize', action='store_true',
                        help='Mode auto-optimisation (expÃ©rimental)')
    
    args = parser.parse_args()
    
    # Validation
    if not args.auto_continue and args.stage is None:
        parser.error("--stage requis (ou utiliser --auto-continue)")
    
    print("\n" + "="*80)
    print("ğŸ“ PLOUTOS CURRICULUM LEARNING (V5 - PERFORMANCE FIX)")
    print("="*80)
    print(f"\nâ° DÃ©but : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # âœ… MODE AUTO-CONTINUE
    if args.auto_continue:
        print("ğŸš€ MODE AUTO-CONTINUE : Stages 1 â†’ 2 â†’ 3")
        print("â±ï¸  DurÃ©e totale : ~1.5 heures (âœ… 10x plus rapide)")
        print("â˜• Parfait pour un cafÃ© !\n")
        
        results = {}
        
        # Stage 1
        print("\n" + "#"*80)
        print("# STAGE 1/3 : MONO-ASSET (SPY)")
        print("#"*80)
        model_path_1, sharpe_1 = train_stage(
            stage_num=1,
            use_transfer_learning=False,
            prev_stage=None,
            auto_optimize=args.auto_optimize
        )
        results['stage1'] = {'model': model_path_1, 'sharpe': sharpe_1}
        
        # Stage 2 avec transfer
        print("\n" + "#"*80)
        print("# STAGE 2/3 : MULTI-ASSET ETFs")
        print("#"*80)
        model_path_2, sharpe_2 = train_stage(
            stage_num=2,
            use_transfer_learning=True,
            prev_stage=1,
            auto_optimize=args.auto_optimize
        )
        results['stage2'] = {'model': model_path_2, 'sharpe': sharpe_2}
        
        # Stage 3 avec transfer
        print("\n" + "#"*80)
        print("# STAGE 3/3 : ACTIONS COMPLEXES")
        print("#"*80)
        model_path_3, sharpe_3 = train_stage(
            stage_num=3,
            use_transfer_learning=True,
            prev_stage=2,
            auto_optimize=args.auto_optimize
        )
        results['stage3'] = {'model': model_path_3, 'sharpe': sharpe_3}
        
        # RÃ©sumÃ© final
        print("\n" + "="*80)
        print("ğŸ† CURRICULUM COMPLET TERMINÃ‰ !")
        print("="*80)
        print(f"\nâ° Fin : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"\nğŸ“ˆ RÃ‰SULTATS FINAUX :\n")
        for stage, data in results.items():
            status = "âœ…" if data['sharpe'] >= CALIBRATED_PARAMS[stage]['target_sharpe'] else "âš ï¸"
            print(f"  {status} {stage.upper()} : Sharpe = {data['sharpe']:.2f} (objectif: {CALIBRATED_PARAMS[stage]['target_sharpe']:.2f})")
            print(f"      ModÃ¨le : {data['model']}.zip")
        
        print(f"\nğŸ¯ MODÃˆLE FINAL : {results['stage3']['model']}.zip")
        print(f"ğŸš€ PrÃªt pour le dÃ©ploiement !\n")
        
    else:
        # âœ… MODE SINGLE STAGE
        print(f"ğŸ“Š Stage : {args.stage}")
        print(f"ğŸ”„ Transfer : {'OUI' if args.transfer else 'NON'}")
        print(f"âš¡ V5 : 10x Faster + Reward Fix + Low Commission")
        if args.transfer and args.from_stage:
            print(f"ğŸ¯ Source : Stage {args.from_stage}")
        print()
        
        model_path, sharpe = train_stage(
            stage_num=args.stage,
            use_transfer_learning=args.transfer,
            prev_stage=args.from_stage,
            auto_optimize=args.auto_optimize
        )
        
        print(f"\nâ° Fin : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"âœ… ModÃ¨le : {model_path}.zip")
        print(f"ğŸ“Š Sharpe : {sharpe:.2f}")
        
        if args.stage < 3 and sharpe > 0:
            print(f"\nğŸ’¡ PROCHAINE Ã‰TAPE :")
            print(f"   python3 scripts/train_curriculum.py --stage {args.stage + 1} --transfer")
