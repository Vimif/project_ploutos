# ğŸš€ GUIDE V4 OPTIMAL - ENTRAÃNEMENT PARFAIT

## ğŸ¯ OBJECTIF

Script d'entraÃ®nement **PARFAIT** basÃ© sur:
- ğŸ“š Recherche acadÃ©mique PPO (OpenAI Spinning Up)
- ğŸ§  ExpÃ©rience V1/V2/V3 (3 mois de tests)
- ğŸ“ˆ Best practices trading bots

---

## âœ¨ NOUVEAUTÃ‰S V4

### **1. Early Stopping** ğŸ›‘

```python
# Stop si pas d'amÃ©lioration aprÃ¨s 5 evals
patience = 5
min_improvement = 0.02  # 2% minimum
```

**Pourquoi ?**
- Ã‰vite overfitting (modÃ¨le apprend par cÅ“ur)
- Gagne du temps (arrÃªte quand converge)
- Meilleure gÃ©nÃ©ralisation

---

### **2. Train/Validation Split** ğŸ“

```python
# 80% train, 20% validation
train_data = data[:80%]
val_data = data[80%:]
```

**Pourquoi ?**
- Ã‰value sur donnÃ©es NON VUES
- DÃ©tecte overfitting prÃ©cocement
- Garantit gÃ©nÃ©ralisation

---

### **3. Learning Rate Scheduler** ğŸ“‰

```python
# DÃ©marre Ã  2.5e-4, finit Ã  2.5e-5
initial_lr = 2.5e-4
final_lr = 2.5e-5  # /10
```

**Pourquoi ?**
- DÃ©but : Exploration rapide (LR haut)
- Fin : Convergence fine (LR bas)
- Meilleure performance finale

---

### **4. Best Model Auto-Save** ğŸ’¾

```python
# Sauvegarde automatique si amÃ©lioration
if new_reward > best_reward:
    model.save("best_model.zip")
```

**Avantage** :
- Garde TOUJOURS meilleur modÃ¨le
- Pas de perte si crash
- Facile Ã  dÃ©ployer

---

### **5. Config AcadÃ©mique Optimale** ğŸ¯

BasÃ©e sur **"Spinning Up in Deep RL"** (OpenAI) :

```python
OPTIMAL_CONFIG = {
    'learning_rate': 2.5e-4,  # OpenAI optimal
    'batch_size': 64,         # Petit = meilleure gÃ©nÃ©ralisation
    'n_epochs': 10,           # Standard PPO
    'net_arch': [512, 512, 256],  # Plus petit = moins overfit
    'ent_coef': 0.005,        # Exploration modÃ©rÃ©e
}
```

---

## ğŸ“Š COMPARAISON V3 vs V4

| Feature | V3 FIXED | **V4 OPTIMAL** |
|---------|----------|----------------|
| **Early stopping** | âŒ | âœ… Oui (patience 5) |
| **Train/Val split** | âŒ | âœ… 80/20 |
| **LR scheduler** | âŒ | âœ… Linear decay |
| **Best model save** | âŒ | âœ… Automatique |
| **Batch size** | 4096 | **64** (meilleur) |
| **Net arch** | [512,512,512] | **[512,512,256]** (optimal) |
| **N envs** | 64 | **32** (stabilitÃ©) |
| **Timesteps** | 10M | **5M** (suffisant) |
| **Commission** | 0.01% | **0.1%** (rÃ©aliste) |
| **Trades/jour** | 30 | **20** (conservative) |

---

## ğŸš€ UTILISATION

### **Installation** 

```bash
cd /root/ai-factory/tmp/project_ploutos
git pull origin main
source /root/ai-factory/venv/bin/activate

# VÃ©rifier GPU
nvidia-smi
```

---

### **EntraÃ®nement Standard (RecommandÃ©)** â­

```bash
nohup python3 scripts/train_v4_optimal.py \
  --config optimal \
  --wandb \
  --project Ploutos_V4_FINAL \
  > logs/train_v4.log 2>&1 &

# Suivre logs
tail -f logs/train_v4.log
```

**DurÃ©e** : 8-10h sur RTX 3080

---

### **EntraÃ®nement Rapide (Test)** ğŸ

```bash
python3 scripts/train_v4_optimal.py \
  --config fast \
  --output models/test_v4.zip
```

**Config Fast** :
- 2M timesteps (au lieu de 5M)
- 16 envs (au lieu de 32)
- DurÃ©e : 3-4h

---

### **EntraÃ®nement QualitÃ© Max** ğŸ†

```bash
nohup python3 scripts/train_v4_optimal.py \
  --config quality \
  --wandb \
  > logs/train_v4_quality.log 2>&1 &
```

**Config Quality** :
- 10M timesteps
- 48 envs
- DurÃ©e : 15-18h

---

### **Tickers Custom**

```bash
python3 scripts/train_v4_optimal.py \
  --tickers AAPL MSFT GOOGL NVDA TSLA SPY QQQ \
  --wandb
```

---

## ğŸ“Š MONITORING

### **Pendant EntraÃ®nement**

```bash
# Logs temps rÃ©el
tail -f logs/train_v4.log

# GPU usage
watch -n 5 nvidia-smi

# Processes
ps aux | grep train_v4
```

---

### **W&B Dashboard**

```
https://wandb.ai/vimif-perso/Ploutos_V4_FINAL
```

**MÃ©triques Ã  surveiller** :
- `train/reward` : Doit augmenter
- `eval/mean_reward` : Validation (important !)
- `train/learning_rate` : Doit dÃ©croitre
- `time/fps` : FPS stable ~150-200

---

## âœ… INDICATEURS DE SUCCÃˆS

### **Pendant EntraÃ®nement**

```bash
âœ… Reward train augmente rÃ©guliÃ¨rement
âœ… Reward validation suit (pas trop d'Ã©cart)
âœ… "NOUVEAU MEILLEUR MODÃˆLE" toutes les 50-100k steps
âœ… Early stopping NE se dÃ©clenche PAS avant 3-4M steps
âœ… GPU usage 80-95%
```

### **Signes de ProblÃ¨me** âš ï¸

```bash
âŒ Reward train stagne ou diminue
âŒ Reward validation << reward train (overfit)
âŒ "Pas d'amÃ©lioration" trop tÃ´t (<2M steps)
âŒ GPU usage <50%
âŒ FPS <100
```

---

## ğŸ¯ OBJECTIFS V4

| MÃ©trique | V3 FIXED Cible | **V4 OPTIMAL Cible** |
|----------|----------------|----------------------|
| **Score 90j** | >90 | **>92** â­ |
| **Score 365j** | >80 | **>85** â­ |
| **Return 90j** | >50% | **>60%** |
| **Return 365j** | >20% | **>25%** |
| **Drawdown** | <8% | **<6%** â­ |
| **Trades/jour** | <30 | **15-25** â­ |
| **Win rate** | >55% | **>58%** â­ |
| **Sharpe** | >1.5 | **>2.0** â­ |

---

## ğŸ’¾ FICHIERS GÃ‰NÃ‰RÃ‰S

```
models/
  ploutos_v4_optimal.zip      # ModÃ¨le final
  ploutos_v4_optimal.json     # Config
  best_model.zip              # â­ MEILLEUR modÃ¨le (utilise celui-ci !)
  best_metrics.json           # MÃ©triques du meilleur
  checkpoints/
    ploutos_v4_optimal_100000_steps.zip
    ploutos_v4_optimal_200000_steps.zip
    ...
```

**âš ï¸ IMPORTANT** : Utilise **`best_model.zip`**, pas le final !

---

## ğŸ§ª BACKTEST

### **AprÃ¨s EntraÃ®nement**

```bash
# Backtest 90j (rapide)
python3 scripts/backtest_reliability.py \
  --model models/best_model.zip \
  --days 90 \
  --episodes 5

# Backtest 365j (complet)
python3 scripts/backtest_reliability.py \
  --model models/best_model.zip \
  --days 365 \
  --episodes 10
```

**CritÃ¨res Validation** :
```
âœ… Score 90j > 92
âœ… Score 365j > 85
âœ… Return 365j > 25%
âœ… Drawdown < 6%
âœ… Trades/jour 15-25
```

---

## ğŸ› ï¸ TROUBLESHOOTING

### **"Out of Memory" (GPU)**

```bash
# RÃ©duire envs
python3 scripts/train_v4_optimal.py --config fast  # 16 envs au lieu de 32

# Ou modifier directement
# Dans train_v4_optimal.py ligne 46:
'n_envs': 16,  # Au lieu de 32
```

---

### **"Pas d'amÃ©lioration" trop tÃ´t**

```python
# Augmenter patience
# Ligne 52:
'patience': 10,  # Au lieu de 5
```

---

### **Training trop lent (FPS <100)**

```bash
# VÃ©rifier GPU usage
nvidia-smi

# Si GPU pas utilisÃ© :
export CUDA_VISIBLE_DEVICES=0

# Ou forcer CPU (plus lent mais marche)
python3 scripts/train_v4_optimal.py --device cpu
```

---

### **W&B ne se connecte pas**

```bash
# Login W&B
wandb login

# Ou dÃ©sactiver
python3 scripts/train_v4_optimal.py  # Sans --wandb
```

---

## ğŸ“š RÃ‰FÃ‰RENCES

### **Recherche AcadÃ©mique**

1. **PPO** : [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347)
2. **Spinning Up** : [OpenAI Deep RL Guide](https://spinningup.openai.com/)
3. **RL Trading** : [Deep Reinforcement Learning for Trading](https://arxiv.org/abs/1911.10107)

### **Best Practices**

- Batch size petit (64) pour gÃ©nÃ©ralisation
- LR decay pour convergence fine
- Train/val split pour dÃ©tecter overfit
- Early stopping pour gagner temps
- Commission rÃ©aliste (0.1%) pour rÃ©alisme

---

## ğŸ† CHECKLIST COMPLÃˆTE

### **Avant EntraÃ®nement**

- [ ] GPU fonctionnel (`nvidia-smi`)
- [ ] DonnÃ©es disponibles (10 tickers minimum)
- [ ] Espace disque >10GB
- [ ] W&B login (optionnel)
- [ ] Logs directory existe

### **Pendant EntraÃ®nement**

- [ ] GPU usage 80-95%
- [ ] FPS stable 150-200
- [ ] Reward augmente
- [ ] "NOUVEAU MEILLEUR MODÃˆLE" rÃ©gulier
- [ ] Pas crash

### **AprÃ¨s EntraÃ®nement**

- [ ] `best_model.zip` existe
- [ ] `best_metrics.json` existe
- [ ] Backtest 90j score >92
- [ ] Backtest 365j score >85
- [ ] Trades/jour 15-25
- [ ] Drawdown <6%

---

## ğŸš€ QUICK START

```bash
# Clone
cd /root/ai-factory/tmp/project_ploutos
git pull

# Activate
source /root/ai-factory/venv/bin/activate

# Train
nohup python3 scripts/train_v4_optimal.py \
  --wandb \
  --project Ploutos_V4_FINAL \
  > logs/train_v4.log 2>&1 &

# Monitor
tail -f logs/train_v4.log

# Wait 8-10h...

# Backtest
python3 scripts/backtest_reliability.py \
  --model models/best_model.zip \
  --days 365 \
  --episodes 10

# Si OK :
# â†’ DÃ©ployer sur VPS
# â†’ Paper trading 7 jours
# â†’ LIVE ğŸš€
```

---

**Date** : 9 DÃ©cembre 2025  
**Version** : V4 OPTIMAL  
**Status** : âœ… PRÃŠT Ã€ UTILISER  
**Auteur** : Ploutos AI Team
