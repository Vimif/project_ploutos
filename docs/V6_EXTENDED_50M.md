# ğŸš€ Ploutos V6 Extended : 50M TIMESTEPS

## ğŸ¯ Pourquoi 50M Steps ?

### ğŸ“‰ Analyse Performance V6 Standard (15M)

**RÃ©sultats actuels :**
```
BUY Timing:  23% good (vs 15% V4)  â†’ +50% amÃ©lioration MAIS insuffisant
SELL Timing: 85% good               â†’ Excellent
Return:      +15.87%                â†’ Bon
Objectif:    50%+ BUY good          â†’ Non atteint
```

### ğŸ§  HypothÃ¨se : Underfitting

**ComplexitÃ© du problÃ¨me :**
```
Tickers:       15
Features:      85 par ticker
Dimensions:    1,275 (15 Ã— 85)
ParamÃ¨tres:    ~2.5M (rÃ©seau)

Formule empirique: Timesteps optimal â‰ˆ Tickers Ã— Features Ã— 5000
15 Ã— 85 Ã— 5000 = 6.4M minimum

RÃ¨gle sÃ»r: 5-8x le minimum = 30M-50M
```

**Conclusion :** 15M est probablement **insuffisant** pour convergence complÃ¨te.

---

## âš™ï¸ Configuration V6 Extended

### Modifications vs V6 Standard

| ParamÃ¨tre | V6 Standard | V6 Extended 50M | Raison |
|-----------|-------------|-----------------|--------|
| **total_timesteps** | 15M | **50M** | Convergence complÃ¨te |
| **ent_coef** | 0.10 | **0.08** | Moins exploration, plus exploitation |
| **checkpoint_freq** | 50k | **100k** | Moins d'I/O disque |
| **eval_freq** | 10k | **20k** | Moins d'Ã©vals, plus de train |
| **early_stopping** | Non | **Oui (25 evals)** | Protection overfit |

### ğŸ›¡ï¸ Protection Anti-Overfitting

**1. Early Stopping**
```yaml
early_stopping:
  enabled: true
  max_no_improvement_evals: 25  # Stop si stagnation 500k steps
  min_evals: 100                # Attendre 2M steps minimum
```

**2. RÃ©gularisation naturelle**
- `ent_coef: 0.08` - Entropy force exploration
- `max_grad_norm: 0.5` - Clip gradients
- `target_kl: 0.02` - Limite divergence policy

**3. Monitoring continu**
- Ã‰valuation tous les 20k steps
- Best model sauvegardÃ© automatiquement
- Checkpoints tous les 100k steps

---

## ğŸš€ Lancer l'EntraÃ®nement

### PrÃ©requis

**Hardware recommandÃ© :**
- GPU : RTX 3080 / RTX 4070 ou supÃ©rieur
- RAM : 32GB+
- Disque : 10GB libres

**DurÃ©e estimÃ©e :**

| GPU | DurÃ©e 50M steps |
|-----|-------------------|
| **RTX 3080** | **15-18h** â±ï¸ |
| **RTX 4090** | 8-10h âš¡ |
| **CPU 16 cores** | 120-150h ğŸ¢ |

### Commandes

```bash
cd /root/ai-factory/tmp/project_ploutos

# RÃ©cupÃ©rer derniers fichiers
git pull origin main

# Rendre exÃ©cutable
chmod +x scripts/train_v6_extended_50m.sh

# âœ… RECOMMANDÃ‰: Mode background
bash scripts/train_v6_extended_50m.sh --nohup

# Suivre progression
tail -f logs/v6_extended_50m/training_*.log

# Monitorer GPU
watch -n 1 nvidia-smi

# TensorBoard
tensorboard --logdir runs/v6_extended_50m/ --port 6006
```

### ArrÃªter l'EntraÃ®nement

```bash
# Trouver le processus
ps aux | grep train_v6_extended_50m

# ArrÃªter proprement (sauvegarde dernier checkpoint)
pkill -f train_v6_extended_50m

# Ou kill brutal (pas recommandÃ©)
kill -9 <PID>
```

---

## ğŸ“Š Monitoring

### Logs Ã  Surveiller

**1. Training logs**
```bash
tail -f logs/v6_extended_50m/training_*.log

# Chercher:
# - "rollout/ep_rew_mean" : reward moyen
# - "train/entropy_loss" : exploration
# - "train/policy_loss" : convergence
```

**2. TensorBoard**
```bash
tensorboard --logdir runs/v6_extended_50m/

# Ouvrir: http://localhost:6006
# Surveiller:
# - rollout/ep_rew_mean : doit monter progressivement
# - train/entropy_loss : doit diminuer lÃ©gÃ¨rement
# - eval/mean_reward : NE DOIT PAS diverger de train
```

**3. GPU Usage**
```bash
watch -n 1 nvidia-smi

# VÃ©rifier:
# - GPU Util : 90-100% (bon)
# - Memory : 8-12GB utilisÃ©s (normal)
# - Temperature : <85Â°C (ok)
```

### ğŸš¨ Signaux d'Alerte

**âš ï¸ Overfitting dÃ©tectÃ© :**
```
train/ep_rew_mean : 150 (monte)
eval/mean_reward  : 80  (descend ou stagne)
â†’ GAP se creuse = OVERFITTING
â†’ Early stopping va s'activer
```

**âš ï¸ InstabilitÃ© :**
```
train/policy_loss : valeurs erratiques
train/value_loss  : explose
â†’ Learning rate trop Ã©levÃ©
â†’ ConsidÃ©rer rÃ©duire LR
```

**âœ… Tout va bien :**
```
train/ep_rew_mean : monte doucement
eval/mean_reward  : suit train (gap <20%)
train/entropy_loss : diminue lÃ©gÃ¨rement
â†’ Convergence saine
```

---

## ğŸ¯ Objectifs V6 Extended 50M

### MÃ©triques Cibles

| MÃ©trique | V4 | V6 15M | **V6 50M Cible** |
|----------|-------|--------|------------------|
| **BUY Quality** | 15% | 23% | **45-60%** ğŸ¯ |
| **SELL Quality** | 60% | 85% | **80-90%** âœ… |
| **Return** | +7.4% | +15.9% | **+20%+** ğŸš€ |
| **Sharpe Ratio** | 1.59 | 4.27 | **>4.0** âœ… |
| **Win Rate** | 50% | 43% | **55-65%** ğŸ¯ |
| **Max Drawdown** | 4.5% | 5.6% | **<7%** âœ… |

### CritÃ¨res de SuccÃ¨s

âœ… **SuccÃ¨s COMPLET** : BUY quality â‰¥ 50%  
âœ… **SuccÃ¨s PARTIEL** : BUY quality â‰¥ 35%  
âŒ **Ã‰chec** : BUY quality < 30%  

---

## ğŸ§ª Tester le ModÃ¨le

### AprÃ¨s EntraÃ®nement

**1. Backtest de performance**
```bash
python scripts/backtest_v6.py \
    --model models/v6_extended_50m_best/best_model.zip \
    --episodes 10 \
    --days 90
```

**2. Analyse timing (CRITIQUE)**
```bash
python scripts/analyze_why_fails_v6.py \
    --model models/v6_extended_50m_best/best_model.zip
```

**Chercher dans les rÃ©sultats :**
```
ğŸ¯ Analyse qualitÃ© du timing...

  ğŸ“ˆ BUYs:
    âœ… Good (buy low):  ??? (??.?%)   â† DOIT ÃŠTRE â‰¥ 45%
    âŒ Bad (buy high):  ??? (??.?%)
```

**3. Comparaison vs V6 15M**
```bash
# Tester les 2 modÃ¨les sur mÃªmes donnÃ©es
python scripts/compare_models.py \
    --model1 models/v6_better_timing_best/best_model.zip \
    --model2 models/v6_extended_50m_best/best_model.zip
```

---

## ğŸ”„ Si Ã‰chec (BUY Quality < 30%)

### Plan B : Approches Alternatives

**1. Simplifier drastiquement (V7 Ultra-Simple)**
- 1 ticker (NVDA uniquement)
- 10-15 features max
- Reward = PnL pur
- 10M timesteps

**2. Reward Shaping plus agressif**
- Bonus explicite +5.0 pour good BUY timing
- PÃ©nalitÃ© -2.0 pour bad BUY timing
- Utiliser lookahead pendant training

**3. Approche Hybride RL + Rules**
- RL dÃ©cide **QUAND** trader (timing)
- RÃ¨gles fixes **COMBIEN** trader (sizing)
- Combine meilleur des 2 mondes

**4. Algorithmes alternatifs**
- DQN (Discrete actions, plus simple)
- SAC (Soft Actor-Critic, plus stable)
- A2C (Advantage Actor-Critic)

---

## ğŸ“š RÃ©fÃ©rences

### Fichiers ClÃ©s

**Configuration :**
- `config/training_config_v6_extended_50m.yaml`

**Scripts :**
- `training/train_v6_extended_50m.py`
- `scripts/train_v6_extended_50m.sh`
- `scripts/backtest_v6.py`
- `scripts/analyze_why_fails_v6.py`

**Environnement :**
- `core/universal_environment_v6_better_timing.py`
- `core/advanced_features_v2.py`

### Documentation ComplÃ©mentaire

- [Guide V6 Standard](./V6_BETTER_TIMING.md)
- [Features V2 Details](../core/advanced_features_v2.py)
- [Training Best Practices](./TRAINING.md)

---

## â±ï¸ Timeline EstimÃ©e

**RTX 3080 (cas typique) :**

```
HÃ©bergement: 0h    â”€â”€â”€ DÃ©but entraÃ®nement
H+3h:               â”€â”€â”€ ~10M steps (20%)
H+6h:               â”€â”€â”€ ~20M steps (40%)
H+9h:               â”€â”€â”€ ~30M steps (60%)
H+12h:              â”€â”€â”€ ~40M steps (80%)
H+15h:              â”€â”€â”€ ~50M steps (100%) âœ…

Ou early stopping si convergence avant!
```

---

**Date :** December 11, 2025  
**Version :** V6 Extended 50M  
**Status :** âœ… PrÃªt Ã  lancer  
**Objectif :** BUY quality â‰¥ 45%
