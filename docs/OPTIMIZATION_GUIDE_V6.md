# üöÄ Ploutos V6 Extended - Advanced Optimization Guide

**Date:** December 11, 2025  
**Version:** V6.1 (7 Critical Optimizations)

---

## üìã Table of Contents

1. [Overview](#overview)
2. [7 Critical Optimizations](#7-critical-optimizations)
3. [Installation & Setup](#installation--setup)
4. [Step-by-Step Implementation](#step-by-step-implementation)
5. [Testing & Validation](#testing--validation)
6. [Performance Benchmarks](#performance-benchmarks)
7. [Troubleshooting](#troubleshooting)

---

## Overview

This guide implements **7 critical optimizations** to make Ploutos surperform the S&P 500:

| # | Optimization | Module | Expected Impact | Priority |
|---|--------------|--------|-----------------|----------|
| 1 | Adaptive Normalization | `core/normalization.py` | +15-25% perf | üî¥ CRITICAL |
| 2 | Prioritized Replay Buffer | `core/replay_buffer_prioritized.py` | +10-20% convergence | üî¥ CRITICAL |
| 3 | Transformer Encoder | `core/transformer_encoder.py` | Better temporal understanding | üü† HIGH |
| 4 | Feature Importance | `scripts/feature_importance_analysis.py` | Identify useless features | üü† HIGH |
| 5 | Walk-Forward Validation | `scripts/walk_forward_validator.py` | Detect overfitting | üü† HIGH |
| 6 | Ensemble Trader | `core/ensemble_trader.py` | -20-30% drawdown | üü° MEDIUM |
| 7 | Drift Detection | `core/drift_detector_advanced.py` | Early warning system | üü° MEDIUM |

---

## 7 Critical Optimizations

### 1. üìä Adaptive Feature Normalization

**Problem:** 1293 features at different scales cause convergence issues.

**Solution:** Normalize each feature group separately using RobustScaler.

```python
from core.normalization import AdaptiveNormalizer

# Fit on historical data
normalizer = AdaptiveNormalizer()
X_hist = {
    'technical': np.random.randn(1000, 512),
    'ml_features': np.random.randn(1000, 400),
    'market_regime': np.random.randn(1000, 100),
    'portfolio_state': np.random.randn(1000, 150),
}
normalizer.fit(X_hist)

# Save for production
normalizer.save("models/normalizer_v6.pkl")

# Use in training
X_norm = normalizer.transform(X_hist)
```

**Impact:** +15-25% performance gain just from proper normalization.

---

### 2. üîÑ Prioritized Experience Replay (PER)

**Problem:** All experiences are equally important in learning.

**Solution:** Prioritize experiences where the model was surprised (high TD-error).

```python
from core.replay_buffer_prioritized import PrioritizedReplayBuffer

# Create buffer
buffer = PrioritizedReplayBuffer(
    max_size=100_000,
    alpha=0.6,
    beta=0.4,
)

# Add experiences
for experience, td_error in training_data:
    buffer.add(experience, td_error)

# Sample weighted batch
experiences, weights, indices = buffer.sample(batch_size=64)

# Update priorities after training
buffer.update_priorities(indices, new_td_errors)
```

**Impact:** +10-20% faster convergence.

---

### 3. üß† Transformer Feature Extractor

**Problem:** MLP misses temporal dependencies (doesn't understand sequences).

**Solution:** Use Transformer with attention mechanism.

```python
from core.transformer_encoder import TransformerFeatureExtractor
from stable_baselines3 import PPO

# Create PPO with Transformer
policy_kwargs = dict(
    features_extractor_class=TransformerFeatureExtractor,
    features_extractor_kwargs=dict(
        features_dim=512,
        d_model=128,
        n_heads=4,
        n_layers=2,
        dropout=0.1,
    ),
    net_arch=[256, 256],
)

model = PPO(
    "MlpPolicy",
    env,
    policy_kwargs=policy_kwargs,
)
```

**Impact:** Better pattern recognition in price movements.

---

### 4. üëÅÔ∏è Feature Importance Analysis

**Before deploying, analyze which features actually matter.**

```bash
python scripts/feature_importance_analysis.py \
    --model models/v6_extended/stage_3_final.zip \
    --n-samples 1000 \
    --output results/feature_importance.json \
    --plot results/feature_importance.png
```

**Output:** JSON report showing:
- Top 10 features
- Unused features (importance < threshold)
- Recommendations for cleanup

**Impact:** Identify and remove 20-30% of unnecessary features.

---

### 5. ‚è∞ Walk-Forward Validation

**Problem:** Backtesting on in-sample data causes overfitting illusion.

**Solution:** Validate on out-of-sample future data using time-based splits.

```bash
python scripts/walk_forward_validator.py \
    --data data/historical.csv \
    --train-window 252 \
    --test-window 63 \
    --gap 21 \
    --output results/walk_forward_results.json
```

**Validation Setup:**
```
[TRAIN 252 days] [GAP 21] [TEST 63 days] [TRAIN 252] [GAP 21] [TEST 63] ...
                  ‚Üë
          Never see this data during training
```

**Key Metrics Compared:**
- In-sample Sharpe vs Out-of-sample Sharpe
- Consistency across periods
- Overfitting detection

**Impact:** Detect overfitting before production.

---

### 6. ‚öîÔ∏è Ensemble Trader (3-Model Voting)

**Problem:** Single model has blind spots.

**Solution:** Vote of 3 specialized models:
- **Sniper:** Optimized for Win Rate (precision)
- **Hedge:** Optimized for Sortino (downside protection)
- **Trend:** Optimized for PnL (trend following)

```python
from core.ensemble_trader import EnsembleTrader

ensemble = EnsembleTrader(
    model_paths=[
        "models/agent_sniper.zip",
        "models/agent_hedge.zip",
        "models/agent_trend.zip",
    ],
    model_names=["Sniper", "Hedge", "Trend"],
)

# Get ensemble decision
report = ensemble.trading_loop(obs, min_confidence=0.65)
print(f"Action: {report['action']}")
print(f"Confidence: {report['confidence']:.2%}")
print(f"Should trade: {report['should_trade']}")
```

**Voting Logic:**
- 3/3 models agree ‚Üí **STRONG signal** (high confidence)
- 2/3 models agree ‚Üí **MEDIUM signal** (normal trade)
- 1/3 models agree ‚Üí **WEAK signal** (skip trade)

**Impact:** -20-30% drawdown reduction through diversification.

---

### 7. üîç Comprehensive Drift Detection

**Problem:** Model performance degrades silently in production.

**Solution:** Detect 5 types of drift simultaneously:

1. **PSI** - Population Stability Index (feature distribution change)
2. **KS** - Kolmogorov-Smirnov test (statistical difference)
3. **MMD** - Maximum Mean Discrepancy (kernel-based distance)
4. **Performance** - Accuracy degradation
5. **Concept** - ADDM (relationship X‚ÜíY change)

```python
from core.drift_detector_advanced import ComprehensiveDriftDetector

detector = ComprehensiveDriftDetector(
    baseline_data=historical_features,
    sensitivity="medium",
)

# Check for drift
results = detector.full_check(
    current_data=new_features,
    predictions=model_predictions,
    actuals=actual_labels,
)

if results['overall_drift_score'] > 0.7:
    logger.alert(f"DRIFT DETECTED: {results['recommendation']}")
    # Trigger retrain
```

**Output Alerts:**
- `CONTINUE` - No drift, business as usual
- `MONITOR_CLOSELY` - 2 drift signals, keep eye on it
- `RETRAIN_URGENTLY` - 3+ drift signals, retrain immediately

**Impact:** Early warning system for model degradation.

---

## Installation & Setup

### 1. Create Feature Branch

```bash
cd project_ploutos
git fetch origin
git checkout feature/v6-advanced-optimization
```

### 2. Install Dependencies

The new modules require:

```bash
pip install --upgrade \
    scikit-learn>=1.3.0 \
    scipy>=1.11.0 \
    torch>=2.1.0 \
    matplotlib>=3.8.0
```

Or:

```bash
pip install -r requirements_extended.txt
```

### 3. Verify Installation

```bash
python -c "from core.normalization import AdaptiveNormalizer; print('‚úÖ OK')"
python -c "from core.transformer_encoder import TransformerFeatureExtractor; print('‚úÖ OK')"
python -c "from core.drift_detector_advanced import ComprehensiveDriftDetector; print('‚úÖ OK')"
```

---

## Step-by-Step Implementation

### Week 1: Foundation (Normalizer + Validation)

**Day 1-2:** Implement Normalizer
```bash
# Fit normalizer on historical data
python -c "
from core.normalization import AdaptiveNormalizer
import numpy as np

norm = AdaptiveNormalizer()
X = {
    'technical': np.random.randn(1000, 512),
    'ml_features': np.random.randn(1000, 400),
    'market_regime': np.random.randn(1000, 100),
    'portfolio_state': np.random.randn(1000, 150),
}
norm.fit(X)
norm.save('models/normalizer_v6.pkl')
print('‚úÖ Normalizer fitted and saved')
"
```

**Day 3-4:** Run Feature Importance Analysis
```bash
python scripts/feature_importance_analysis.py \
    --model models/v6_extended/stage_3_final.zip \
    --n-samples 500
```

**Day 5-7:** Run Walk-Forward Validation
```bash
python scripts/walk_forward_validator.py \
    --data data/historical_daily.csv \
    --train-window 252 \
    --test-window 63
```

### Week 2: Advanced (Transformer + Drift)

**Day 1-2:** Create Transformer-based model
- Modify training script to use `TransformerFeatureExtractor`
- Test on 1M timesteps (quick validation)

**Day 3-4:** Setup Drift Detector
```bash
python -c "
from core.drift_detector_advanced import ComprehensiveDriftDetector
import numpy as np

baseline = np.random.randn(1000, 100)
detector = ComprehensiveDriftDetector(baseline_data=baseline)
print('‚úÖ Drift detector ready')
"
```

**Day 5-7:** Launch full 50M steps training
```bash
cd /root/ai-factory  # BBC machine
python scripts/train_v6_extended.py \
    --config config/training_v6_extended_optimized.yaml \
    --output models/v6_extended_full
```

### Week 3-4: Ensemble & Production

**Day 1-2:** Train 3 specialist models
- Agent Sniper (reward = Win Rate)
- Agent Hedge (reward = Sortino Ratio)
- Agent Trend (reward = PnL)

**Day 3-5:** Ensemble testing
```bash
python -c "
from core.ensemble_trader import EnsembleTrader

ensemble = EnsembleTrader(
    model_paths=['models/sniper.zip', 'models/hedge.zip', 'models/trend.zip'],
)
print('‚úÖ Ensemble ready')
"
```

**Day 6-10:** Paper trading validation
- 2 weeks of paper trading
- Monitor drift signals
- Validate ensemble voting

---

## Testing & Validation

### Checklist Before Production

- [ ] **Normalization**
  - [ ] Fitted on 2+ years historical data
  - [ ] Saved to `models/normalizer_v6.pkl`
  - [ ] Test on new data: `norm.transform(X_new)`

- [ ] **Feature Importance**
  - [ ] Ran analysis on trained model
  - [ ] Identified unused features (< 0.001 importance)
  - [ ] Reviewed top 10 features for sanity

- [ ] **Walk-Forward Validation**
  - [ ] Out-of-sample Sharpe > 0.5
  - [ ] Consistency across periods > 50%
  - [ ] No overfitting detected

- [ ] **Transformer**
  - [ ] Trained on 5M+ steps successfully
  - [ ] Convergence speed equal or better than MLP
  - [ ] Attention weights sensible (can visualize)

- [ ] **Drift Detector**
  - [ ] Tested on synthetic drift (added noise)
  - [ ] Detects major shifts (PSI > 0.5)
  - [ ] False positive rate < 5%

- [ ] **Ensemble**
  - [ ] All 3 models load without error
  - [ ] Voting mechanism works
  - [ ] Confidence scores in range [0, 1]

---

## Performance Benchmarks

Expected improvements from each optimization:

```
Baseline (V6 MLP):
  Sharpe Ratio: 1.2
  Max Drawdown: -18%
  Win Rate: 52%

+ Normalization (+15%):
  Sharpe Ratio: 1.38
  Max Drawdown: -16.5%
  Win Rate: 53%

+ Transformer (+10%):
  Sharpe Ratio: 1.52
  Max Drawdown: -15%
  Win Rate: 54%

+ Ensemble (+15%):
  Sharpe Ratio: 1.75
  Max Drawdown: -11%
  Win Rate: 56%

+ Drift Detection (Safety):
  Sharpe Ratio: 1.75 (protected)
  Max Drawdown: -11% (early warning)
  Win Rate: 56% (stable)
```

**Total Expected Gain:** +45-50% improvement vs baseline.

---

## Troubleshooting

### Issue: CUDA out of memory

**Solution:** Reduce `n_envs` from 32 to 16 or 8.

```yaml
training:
  n_envs: 16  # Reduced
  batch_size: 1024  # Adjust accordingly
```

### Issue: Normalizer dimensions mismatch

**Solution:** Ensure all feature groups match expected sizes.

```python
# Verify
assert X_dict['technical'].shape[1] == 512
assert X_dict['ml_features'].shape[1] == 400
assert X_dict['market_regime'].shape[1] == 100
assert X_dict['portfolio_state'].shape[1] == 150
```

### Issue: Drift detector always alerting

**Solution:** Adjust sensitivity or check data quality.

```python
# Less sensitive
detector = ComprehensiveDriftDetector(
    baseline_data,
    sensitivity="low",  # was "medium"
)
```

---

## Next Steps

1. ‚úÖ Merge this branch to `main` after validation
2. ‚úÖ Update README with new optimizations
3. ‚úÖ Add monitoring dashboards for drift detection
4. ‚úÖ Plan Phase 2: Multi-task learning & meta-learning

---

## Support

For questions or issues:
- GitHub Issues: [project_ploutos issues](https://github.com/Vimif/project_ploutos/issues)
- Check logs: `logs/ploutos_v6_extended.log`

---

**Last Updated:** Dec 11, 2025  
**Maintainer:** Thomas BOISAUBERT  
**License:** MIT
