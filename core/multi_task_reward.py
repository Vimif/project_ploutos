"""
üéØ MULTI-TASK LEARNING - Strat√©gie d'Entra√Ænement Avanc√©e #1

Optimise plusieurs objectifs simultan√©ment au lieu d'un seul reward.
Gains attendus: +15-20% Sharpe tout en conservant les profits.

Usage:
    from core.multi_task_reward import MultiTaskRewardWrapper
    
    env = UniversalTradingEnv(...)
    env = MultiTaskRewardWrapper(env, weights=[0.5, 0.3, 0.2])
    
    # Le wrapper calcule automatiquement:
    # reward = 0.5√óprofit + 0.3√ósharpe - 0.2√ódrawdown

R√©f√©rences:
    - Meta-Learning: Multi-task crucial pour g√©n√©ralisation
    - Regime-Aware: Pond√©ration dynamique am√©liore adaptabilit√©
"""

import gymnasium as gym
import numpy as np
from collections import deque


class MultiTaskRewardWrapper(gym.Wrapper):
    """
    Wrapper qui transforme le reward simple en reward multi-objectifs.
    
    ‚úÖ COMPATIBLE BACKWARD: Ne modifie PAS l'environnement de base
    ‚úÖ FACILE √Ä ACTIVER: Juste wrapper l'env existant
    ‚úÖ ZERO REGRESSION: Si pas utilis√©, comportement identique
    """
    
    def __init__(
        self,
        env,
        weights=None,
        objectives=None,
        rolling_window=50,
        auto_adjust=False,
        target_sharpe=1.0,
        max_drawdown=0.05
    ):
        """
        Args:
            env: UniversalTradingEnv de base
            weights: [profit_weight, sharpe_weight, drawdown_weight, winrate_weight]
                    Default: [0.5, 0.3, 0.15, 0.05]
            objectives: Liste des objectifs √† optimiser
                       Default: ['profit', 'sharpe', 'drawdown', 'winrate']
            rolling_window: Taille fen√™tre pour calcul Sharpe/drawdown (default: 50)
            auto_adjust: Ajuster poids automatiquement selon performances
            target_sharpe: Sharpe cible pour auto-adjustment
            max_drawdown: Drawdown max accept√©
        """
        super().__init__(env)
        
        # Configuration objectifs
        self.objectives = objectives or ['profit', 'sharpe', 'drawdown', 'winrate']
        
        # Poids par d√©faut (calibr√©s pour trading actions US)
        default_weights = {
            'profit': 0.5,      # 50% focus profit
            'sharpe': 0.3,      # 30% focus risk-adjusted
            'drawdown': 0.15,   # 15% focus protection capital
            'winrate': 0.05     # 5% focus taux succ√®s
        }
        
        if weights is None:
            self.weights = [default_weights[obj] for obj in self.objectives]
        else:
            self.weights = weights
        
        # Normaliser poids
        total = sum(self.weights)
        self.weights = [w / total for w in self.weights]
        
        # Historique pour calculs rolling
        self.rolling_window = rolling_window
        self.returns_history = deque(maxlen=rolling_window)
        self.portfolio_values = deque(maxlen=rolling_window)
        self.trades_outcomes = deque(maxlen=rolling_window)
        
        # Auto-adjustment
        self.auto_adjust = auto_adjust
        self.target_sharpe = target_sharpe
        self.max_drawdown = max_drawdown
        self.adjustment_history = []
        
        # M√©triques √©pisode
        self.episode_rewards_breakdown = []
        self.initial_portfolio_value = None
        self.max_portfolio_value = None
        
        print(f"\n‚ú® Multi-Task Reward activ√©")
        print(f"üìä Objectifs: {self.objectives}")
        print(f"‚öñÔ∏è  Poids: {[f'{w:.2f}' for w in self.weights]}")
        if auto_adjust:
            print(f"üîß Auto-ajustement activ√© (target Sharpe: {target_sharpe})")
        print()
    
    def reset(self, **kwargs):
        """Reset avec tracking multi-task"""
        obs, info = self.env.reset(**kwargs)
        
        # Reset historiques
        self.returns_history.clear()
        self.portfolio_values.clear()
        self.trades_outcomes.clear()
        self.episode_rewards_breakdown = []
        
        # Init valeurs
        self.initial_portfolio_value = self.env.portfolio_value
        self.max_portfolio_value = self.env.portfolio_value
        self.portfolio_values.append(self.env.portfolio_value)
        
        return obs, info
    
    def step(self, action):
        """Step avec reward multi-objectifs"""
        # Valeur AVANT action
        previous_value = self.env.portfolio_value
        
        # Ex√©cuter action dans env de base
        obs, base_reward, terminated, truncated, info = self.env.step(action)
        
        # Valeur APR√àS action
        current_value = self.env.portfolio_value
        
        # Update historiques
        self.portfolio_values.append(current_value)
        self.max_portfolio_value = max(self.max_portfolio_value, current_value)
        
        # Calcul return
        portfolio_return = (current_value - previous_value) / previous_value
        self.returns_history.append(portfolio_return)
        
        # Track trade outcome (si trade effectu√©)
        if len(self.env.trades_history) > len(self.trades_outcomes):
            # Nouveau trade
            trade_profitable = portfolio_return > 0
            self.trades_outcomes.append(1 if trade_profitable else 0)
        
        # ‚ú® CALCUL MULTI-TASK REWARD
        rewards_components = self._calculate_multi_task_rewards(
            base_reward, 
            current_value,
            portfolio_return
        )
        
        # Combinaison pond√©r√©e
        multi_task_reward = sum(
            w * r for w, r in zip(self.weights, rewards_components.values())
        )
        
        # Clip pour stabilit√©
        multi_task_reward = np.clip(multi_task_reward, -0.2, 0.2)
        
        # Store breakdown pour analyse
        self.episode_rewards_breakdown.append({
            'step': self.env.current_step,
            'total': multi_task_reward,
            **rewards_components
        })
        
        # Auto-ajustement poids (si activ√©)
        if self.auto_adjust and len(self.returns_history) >= self.rolling_window:
            self._auto_adjust_weights()
        
        # Enrichir info
        info['multi_task'] = {
            'total_reward': multi_task_reward,
            'components': rewards_components,
            'weights': {obj: w for obj, w in zip(self.objectives, self.weights)},
            'current_sharpe': self._compute_sharpe(),
            'current_drawdown': self._compute_drawdown()
        }
        
        return obs, multi_task_reward, terminated, truncated, info
    
    def _calculate_multi_task_rewards(self, base_reward, current_value, portfolio_return):
        """
        Calcule les rewards pour chaque objectif
        
        Returns:
            dict: {objective: reward_value}
        """
        rewards = {}
        
        for obj in self.objectives:
            if obj == 'profit':
                # Profit brut normalis√©
                rewards['profit'] = base_reward
            
            elif obj == 'sharpe':
                # Sharpe ratio rolling
                if len(self.returns_history) >= 10:
                    sharpe = self._compute_sharpe()
                    # Normaliser: Sharpe 0 = reward 0, Sharpe 1 = reward 0.1
                    rewards['sharpe'] = sharpe * 0.1
                else:
                    rewards['sharpe'] = 0.0
            
            elif obj == 'drawdown':
                # P√©nalit√© drawdown
                drawdown = self._compute_drawdown()
                # P√©nalit√© quadratique si drawdown > seuil
                if drawdown > self.max_drawdown:
                    penalty = -((drawdown - self.max_drawdown) ** 2) * 10
                else:
                    penalty = 0.0
                rewards['drawdown'] = penalty
            
            elif obj == 'winrate':
                # Taux de trades gagnants
                if len(self.trades_outcomes) >= 5:
                    winrate = np.mean(self.trades_outcomes)
                    # Bonus si winrate > 50%
                    rewards['winrate'] = (winrate - 0.5) * 0.1
                else:
                    rewards['winrate'] = 0.0
            
            elif obj == 'calmar':
                # Calmar ratio (return / max drawdown)
                drawdown = self._compute_drawdown()
                if drawdown > 0.01:
                    total_return = (current_value - self.initial_portfolio_value) / self.initial_portfolio_value
                    calmar = total_return / drawdown
                    rewards['calmar'] = calmar * 0.05
                else:
                    rewards['calmar'] = 0.0
            
            elif obj == 'sortino':
                # Sortino ratio (downside risk)
                if len(self.returns_history) >= 10:
                    sortino = self._compute_sortino()
                    rewards['sortino'] = sortino * 0.1
                else:
                    rewards['sortino'] = 0.0
        
        return rewards
    
    def _compute_sharpe(self):
        """Calcule Sharpe ratio rolling"""
        if len(self.returns_history) < 2:
            return 0.0
        
        returns = np.array(self.returns_history)
        mean_return = np.mean(returns)
        std_return = np.std(returns)
        
        if std_return < 1e-6:
            return 0.0
        
        # Annualis√© (252 trading days)
        sharpe = (mean_return / std_return) * np.sqrt(252)
        return float(sharpe)
    
    def _compute_sortino(self):
        """Calcule Sortino ratio (downside risk seulement)"""
        if len(self.returns_history) < 2:
            return 0.0
        
        returns = np.array(self.returns_history)
        mean_return = np.mean(returns)
        
        # Downside deviation
        downside_returns = returns[returns < 0]
        if len(downside_returns) == 0:
            return 0.0
        
        downside_std = np.std(downside_returns)
        
        if downside_std < 1e-6:
            return 0.0
        
        sortino = (mean_return / downside_std) * np.sqrt(252)
        return float(sortino)
    
    def _compute_drawdown(self):
        """Calcule drawdown actuel"""
        if self.max_portfolio_value <= 0:
            return 0.0
        
        current_value = self.env.portfolio_value
        drawdown = (self.max_portfolio_value - current_value) / self.max_portfolio_value
        
        return max(0.0, float(drawdown))
    
    def _auto_adjust_weights(self):
        """
        Auto-ajuste les poids selon performances actuelles
        
        Logique:
            - Si Sharpe < target ‚Üí augmente poids Sharpe
            - Si Drawdown > max ‚Üí augmente poids Drawdown
            - Si Winrate < 40% ‚Üí augmente poids Winrate
        """
        current_sharpe = self._compute_sharpe()
        current_drawdown = self._compute_drawdown()
        current_winrate = np.mean(self.trades_outcomes) if len(self.trades_outcomes) >= 5 else 0.5
        
        adjustments = {}
        
        # Ajustement Sharpe
        if 'sharpe' in self.objectives:
            idx = self.objectives.index('sharpe')
            if current_sharpe < self.target_sharpe:
                self.weights[idx] = min(self.weights[idx] + 0.02, 0.5)
                adjustments['sharpe'] = '+0.02'
        
        # Ajustement Drawdown
        if 'drawdown' in self.objectives:
            idx = self.objectives.index('drawdown')
            if current_drawdown > self.max_drawdown:
                self.weights[idx] = min(self.weights[idx] + 0.03, 0.4)
                adjustments['drawdown'] = '+0.03'
        
        # Ajustement Winrate
        if 'winrate' in self.objectives:
            idx = self.objectives.index('winrate')
            if current_winrate < 0.4:
                self.weights[idx] = min(self.weights[idx] + 0.01, 0.2)
                adjustments['winrate'] = '+0.01'
        
        # Re-normaliser
        total = sum(self.weights)
        self.weights = [w / total for w in self.weights]
        
        # Log si ajustement
        if adjustments:
            self.adjustment_history.append({
                'step': self.env.current_step,
                'adjustments': adjustments,
                'new_weights': self.weights.copy(),
                'sharpe': current_sharpe,
                'drawdown': current_drawdown,
                'winrate': current_winrate
            })
    
    def get_episode_summary(self):
        """R√©cup√®re r√©sum√© de l'√©pisode avec breakdown multi-task"""
        if len(self.episode_rewards_breakdown) == 0:
            return {}
        
        import pandas as pd
        df = pd.DataFrame(self.episode_rewards_breakdown)
        
        summary = {
            'total_reward': df['total'].sum(),
            'avg_reward_per_step': df['total'].mean(),
            'components_sum': {
                obj: df[obj].sum() if obj in df.columns else 0.0
                for obj in self.objectives
            },
            'final_sharpe': self._compute_sharpe(),
            'final_drawdown': self._compute_drawdown(),
            'final_winrate': np.mean(self.trades_outcomes) if len(self.trades_outcomes) > 0 else 0.0,
            'final_weights': {obj: w for obj, w in zip(self.objectives, self.weights)},
            'n_adjustments': len(self.adjustment_history)
        }
        
        return summary


# ============================================================================
# EXEMPLES D'USAGE
# ============================================================================

def example_basic_usage():
    """Exemple basique: wrapper simple"""
    from core.universal_environment import UniversalTradingEnv
    from core.data_fetcher import DataFetcher
    
    # Cr√©er env classique
    fetcher = DataFetcher()
    data = fetcher.fetch_data(['SPY'], period='2y', interval='1h')
    
    base_env = UniversalTradingEnv(data, initial_balance=100000)
    
    # ‚ú® Wrapper multi-task (AUCUNE modification du code existant)
    env = MultiTaskRewardWrapper(
        base_env,
        weights=[0.5, 0.3, 0.2],  # profit, sharpe, drawdown
        objectives=['profit', 'sharpe', 'drawdown']
    )
    
    # Utiliser normalement
    obs, info = env.reset()
    
    for _ in range(100):
        action = env.action_space.sample()
        obs, reward, done, truncated, info = env.step(action)
        
        # Info enrichi avec breakdown
        print(f"Reward: {reward:.4f} | Components: {info['multi_task']['components']}")
        
        if done or truncated:
            break
    
    # R√©sum√© √©pisode
    summary = env.get_episode_summary()
    print(f"\nüìä √âpisode termin√©:")
    print(f"   Total reward: {summary['total_reward']:.4f}")
    print(f"   Final Sharpe: {summary['final_sharpe']:.2f}")
    print(f"   Final Drawdown: {summary['final_drawdown']:.2%}")


def example_auto_adjust():
    """Exemple avec auto-ajustement des poids"""
    from core.universal_environment import UniversalTradingEnv
    from core.data_fetcher import DataFetcher
    
    fetcher = DataFetcher()
    data = fetcher.fetch_data(['SPY', 'QQQ'], period='2y', interval='1h')
    
    base_env = UniversalTradingEnv(data, initial_balance=100000)
    
    # ‚ú® Auto-ajustement activ√©
    env = MultiTaskRewardWrapper(
        base_env,
        auto_adjust=True,
        target_sharpe=1.0,
        max_drawdown=0.05
    )
    
    obs, info = env.reset()
    
    for _ in range(1000):
        action = env.action_space.sample()
        obs, reward, done, truncated, info = env.step(action)
        
        if done or truncated:
            break
    
    # Historique ajustements
    print(f"\nüîß {len(env.adjustment_history)} ajustements de poids effectu√©s")
    for adj in env.adjustment_history[-3:]:
        print(f"   Step {adj['step']}: {adj['adjustments']}")


if __name__ == '__main__':
    print("=" * 80)
    print("üéØ MULTI-TASK LEARNING - Tests")
    print("=" * 80)
    
    print("\n1Ô∏è‚É£  Test basique...")
    example_basic_usage()
    
    print("\n2Ô∏è‚É£  Test auto-adjustment...")
    example_auto_adjust()
    
    print("\n‚úÖ Tests termin√©s !")
