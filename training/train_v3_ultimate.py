#!/usr/bin/env python3
# training/train_v3_ultimate.py
"""Script d'Entra√Ænement V3 ULTIMATE - Ploutos Trading IA

Optimisations:
- Environnement V4 ultra-r√©aliste
- 50+ features avanc√©es
- Architecture neuronale profonde
- Hyperparams optimis√©s
- Data augmentation
- Callbacks avanc√©s
- Weights & Biases tracking
- Multi-GPU support
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import os
import yaml
import wandb
import torch
import numpy as np
from datetime import datetime
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize
from stable_baselines3.common.callbacks import (
    CheckpointCallback, EvalCallback, StopTrainingOnNoModelImprovement
)
from stable_baselines3.common.monitor import Monitor

from core.universal_environment_v4 import UniversalTradingEnvV4
from core.data_fetcher import download_data
from core.utils import setup_logging

logger = setup_logging(__name__, 'training_v3.log')


class WandbCallback:
    """Callback pour logger sur Weights & Biases"""
    
    def __init__(self, verbose=0):
        self.verbose = verbose
    
    def __call__(self, locals_, globals_):
        # Log m√©triques
        if 'infos' in locals_ and len(locals_['infos']) > 0:
            info = locals_['infos'][0]
            
            if 'equity' in info:
                wandb.log({
                    'equity': info['equity'],
                    'total_return': info.get('total_return', 0),
                    'total_trades': info.get('total_trades', 0)
                })
        
        return True


def load_config(config_path: str = 'config/training_config_v3.yaml') -> dict:
    """Charger la configuration"""
    if not os.path.exists(config_path):
        logger.warning(f"Config {config_path} non trouv√©, utilisation config par d√©faut")
        return get_default_config()
    
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    return config


def get_default_config() -> dict:
    """Configuration par d√©faut"""
    return {
        'training': {
            'total_timesteps': 10_000_000,
            'n_envs': 16,
            'batch_size': 8192,
            'n_steps': 4096,
            'n_epochs': 20,
            'learning_rate': 5e-5,
            'gamma': 0.995,
            'gae_lambda': 0.98,
            'clip_range': 0.2,
            'ent_coef': 0.01,
            'vf_coef': 0.5,
            'max_grad_norm': 0.5,
            'target_kl': 0.015
        },
        'environment': {
            'initial_balance': 100000,
            'commission': 0.0,
            'sec_fee': 0.0000221,
            'finra_taf': 0.000145,
            'max_steps': 3000,
            'buy_pct': 0.15,
            'slippage_model': 'realistic',
            'spread_bps': 2.0,
            'market_impact_factor': 0.0001,
            'max_position_pct': 0.25,
            'reward_scaling': 1.0,
            'use_sharpe_penalty': True,
            'use_drawdown_penalty': True,
            'max_trades_per_day': 4,
            'min_holding_period': 5
        },
        'data': {
            'tickers': [
                'NVDA', 'MSFT', 'AAPL', 'GOOGL', 'AMZN',
                'SPY', 'QQQ', 'VOO', 'XLE', 'XLF'
            ],
            'period': '5y',
            'interval': '1h'
        },
        'network': {
            'net_arch': [1024, 512, 256],
            'activation_fn': 'tanh'
        },
        'wandb': {
            'enabled': True,
            'project': 'Ploutos_Trading_V3_ULTIMATE',
            'entity': None
        }
    }


def make_env(data, config, rank):
    """Cr√©er un environnement"""
    def _init():
        env = UniversalTradingEnvV4(
            data=data,
            **config['environment']
        )
        env = Monitor(env)
        return env
    
    return _init


def train_ultimate_model(config_path: str = None):
    """
    Entra√Ænement V3 ULTIMATE
    
    Args:
        config_path: Chemin vers fichier config YAML
    """
    logger.info("="*70)
    logger.info("üöÄ D√âMARRAGE ENTRA√éNEMENT V3 ULTIMATE")
    logger.info("="*70)
    
    # 1. Charger config
    config = load_config(config_path) if config_path else get_default_config()
    logger.info(f"‚úÖ Configuration charg√©e")
    
    # 2. Setup Weights & Biases
    if config['wandb']['enabled']:
        wandb.init(
            project=config['wandb']['project'],
            entity=config['wandb']['entity'],
            config=config,
            name=f"ploutos_v3_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
        logger.info("‚úÖ Weights & Biases initialis√©")
    
    # 3. T√©l√©charger donn√©es
    logger.info("üìä T√©l√©chargement des donn√©es...")
    data = download_data(
        tickers=config['data']['tickers'],
        period=config['data']['period'],
        interval=config['data']['interval']
    )
    logger.info(f"‚úÖ {len(data)} tickers charg√©s")
    
    # 4. Cr√©er environnements parall√®les
    logger.info(f"üè≠ Cr√©ation de {config['training']['n_envs']} environnements parall√®les...")
    
    envs = SubprocVecEnv([
        make_env(data, config, i)
        for i in range(config['training']['n_envs'])
    ])
    
    # Normalisation
    envs = VecNormalize(
        envs,
        norm_obs=True,
        norm_reward=True,
        clip_obs=10.0,
        clip_reward=10.0,
        gamma=config['training']['gamma']
    )
    
    logger.info("‚úÖ Environnements cr√©√©s et normalis√©s")
    
    # 5. Cr√©er mod√®le PPO
    logger.info("üß† Cr√©ation du mod√®le PPO...")
    
    # Architecture
    policy_kwargs = {
        'net_arch': [
            {'pi': config['network']['net_arch'], 'vf': config['network']['net_arch']}
        ],
        'activation_fn': torch.nn.Tanh if config['network']['activation_fn'] == 'tanh' else torch.nn.ReLU
    }
    
    # Device
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"üíª Device: {device}")
    
    model = PPO(
        'MlpPolicy',
        envs,
        learning_rate=config['training']['learning_rate'],
        n_steps=config['training']['n_steps'],
        batch_size=config['training']['batch_size'],
        n_epochs=config['training']['n_epochs'],
        gamma=config['training']['gamma'],
        gae_lambda=config['training']['gae_lambda'],
        clip_range=config['training']['clip_range'],
        ent_coef=config['training']['ent_coef'],
        vf_coef=config['training']['vf_coef'],
        max_grad_norm=config['training']['max_grad_norm'],
        target_kl=config['training']['target_kl'],
        policy_kwargs=policy_kwargs,
        verbose=1,
        device=device,
        tensorboard_log='./runs/v3_ultimate/'
    )
    
    logger.info("‚úÖ Mod√®le cr√©√©")
    logger.info(f"   Architecture: {config['network']['net_arch']}")
    logger.info(f"   Params: {sum(p.numel() for p in model.policy.parameters()):,}")
    
    # 6. Callbacks
    logger.info("üìé Configuration des callbacks...")
    
    checkpoint_callback = CheckpointCallback(
        save_freq=50000,
        save_path='./models/v3_checkpoints/',
        name_prefix='ploutos_v3'
    )
    
    stop_callback = StopTrainingOnNoModelImprovement(
        max_no_improvement_evals=10,
        min_evals=50,
        verbose=1
    )
    
    eval_callback = EvalCallback(
        envs,
        callback_after_eval=stop_callback,
        eval_freq=10000,
        n_eval_episodes=5,
        best_model_save_path='./models/v3_best/',
        log_path='./logs/v3_eval/',
        deterministic=True,
        render=False,
        verbose=1
    )
    
    callbacks = [checkpoint_callback, eval_callback]
    
    logger.info("‚úÖ Callbacks configur√©s")
    
    # 7. ENTRA√éNEMENT
    logger.info("="*70)
    logger.info("üèãÔ∏è  D√âBUT DE L'ENTRA√éNEMENT")
    logger.info("="*70)
    logger.info(f"Total timesteps: {config['training']['total_timesteps']:,}")
    logger.info(f"Batch size: {config['training']['batch_size']:,}")
    logger.info(f"Updates: {config['training']['total_timesteps'] // (config['training']['n_steps'] * config['training']['n_envs']):,}")
    logger.info("="*70)
    
    try:
        model.learn(
            total_timesteps=config['training']['total_timesteps'],
            callback=callbacks,
            progress_bar=True
        )
        
        logger.info("‚úÖ Entra√Ænement termin√© avec succ√®s")
        
    except KeyboardInterrupt:
        logger.warning("‚ö†Ô∏è  Entra√Ænement interrompu par l'utilisateur")
    
    except Exception as e:
        logger.error(f"‚ùå Erreur pendant l'entra√Ænement: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 8. Sauvegarder mod√®le final
    final_model_path = f"models/v3_ultimate/ploutos_v3_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
    os.makedirs(os.path.dirname(final_model_path), exist_ok=True)
    
    model.save(final_model_path)
    logger.info(f"‚úÖ Mod√®le final sauvegard√©: {final_model_path}")
    
    # Sauvegarder VecNormalize
    envs.save(final_model_path.replace('.zip', '_vecnormalize.pkl'))
    logger.info("‚úÖ VecNormalize sauvegard√©")
    
    # 9. Sauvegarder config
    import json
    config_save_path = final_model_path.replace('.zip', '_config.json')
    with open(config_save_path, 'w') as f:
        json.dump(config, f, indent=2)
    logger.info(f"‚úÖ Config sauvegard√©e: {config_save_path}")
    
    # 10. Fermer W&B
    if config['wandb']['enabled']:
        wandb.finish()
    
    logger.info("="*70)
    logger.info("üéâ ENTRA√éNEMENT V3 ULTIMATE TERMIN√â")
    logger.info("="*70)
    
    return model, envs


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='Entra√Ænement V3 ULTIMATE')
    parser.add_argument('--config', type=str, default=None, help='Chemin config YAML')
    
    args = parser.parse_args()
    
    train_ultimate_model(config_path=args.config)
